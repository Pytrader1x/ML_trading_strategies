# Experiment Configuration - v2_exit_guards
# =========================================
# Counterfactual rewards instead of hard guards

meta:
  version: "v2"
  name: "counterfactual_rewards"
  description: "Use counterfactual rewards to steer exits instead of hard guards"
  created: "2026-01-25T09:00:00Z"
  author: "william"
  parent_version: "v1_baseline"
  status: "pending"

# ===========================================================================
# KEY INNOVATION: COUNTERFACTUAL REWARDS
# ===========================================================================
#
# PROBLEM: v1 model exits at Bar 0 with ~0 pips to avoid time penalty
#
# REJECTED SOLUTION: Hard guards (block exits below threshold)
#   - Problem: Blocks legitimate defensive exits
#
# NEW SOLUTION: Counterfactual rewards
#   - When model exits, look at what happens AFTER
#   - If price drops → defensive bonus (good exit, avoided loss)
#   - If price rises → opportunity cost (bad exit, missed gain)
#   - Equal weights → Sharpe-optimized balance
#
# This naturally teaches:
#   - Exit at 0 pips, price goes up → BAD (opportunity cost)
#   - Exit at +10 pips, price drops → GOOD (defensive bonus)
#
# ===========================================================================

training:
  train_episodes: "data/episodes_train_2005_2021.pkl"
  test_episodes: "data/episodes_test_2022_2025.pkl"

  ppo:
    n_envs: 64
    max_episode_length: 200
    state_dim: 25
    action_dim: 5
    gamma: 0.99
    gae_lambda: 0.95
    clip_epsilon: 0.2
    target_kl: 0.015
    learning_rate: 0.0003
    lr_end: 0.00001
    lr_schedule: "linear"
    n_epochs: 10
    batch_size: 2048
    n_steps: 2048
    total_timesteps: 10000000
    value_coef: 0.5
    max_grad_norm: 0.5
    entropy_coef_start: 0.05
    entropy_coef_end: 0.005
    entropy_anneal_steps: 7000000
    hidden_dims: [256, 256]
    use_layer_norm: true
    dropout: 0.0

  reward:
    w_realized: 1.0
    w_mtm: 0.1
    risk_coef: 0.3
    dd_threshold: 0.1
    time_coef: 0.002               # Reduced from 0.005

    # COUNTERFACTUAL REWARDS (new)
    regret_coef: 0.5               # Opportunity cost for missed gains
    defensive_coef: 0.5            # Bonus for avoiding future losses
    counterfactual_lookforward: 20 # Bars to look ahead (~5 hours)
    exit_cost: 0.0001              # Tiny transaction cost (~0.1 pip)

    # REMOVED: Hard guards
    # min_profit_for_exit: REMOVED
    # min_bars_for_exit: REMOVED
    # min_profit_for_partial: REMOVED
    # min_bars_for_partial: REMOVED
    # invalid_action_penalty: REMOVED

    tighten_sl_cost: 0.001
    trail_sl_cost: 0.0005
    reward_scale: 100.0

  device: "cuda"
  wandb_project: "rl-exit-optimizer"

evaluation:
  instruments: ["AUDUSD"]
  timeframes: ["15M"]
  test_range: "2022-01-01 to 2025-01-01"
  train_range: "2005-01-01 to 2021-12-31"

results_summary:
  sharpe_ratio: null
  return_pct: null
  max_drawdown_pct: null
  win_rate: null
  total_trades: null
  profit_factor: null
  cagr: null

changes_from_previous:
  - "REMOVED: Hard guards on EXIT and PARTIAL"
  - "ADDED: Counterfactual defensive bonus (avoid future loss)"
  - "ADDED: Counterfactual opportunity cost (missed future gain)"
  - "ADDED: Tiny exit cost (0.0001) to discourage wasteful exits"
  - "Reduced time_coef: 0.002 (was 0.005)"
  - "Equal defensive/regret weights for Sharpe optimization"

hypothesis: |
  With counterfactual rewards, the model learns:
  1. Exit at Bar 0 with 0 pips, price rises → penalized (missed opportunity)
  2. Exit at +10 pips, price then drops → rewarded (defensive)
  3. Exit at +10 pips, price continues up → slightly penalized (premature)

  Expected improvements over v1:
  - More nuanced exit decisions based on market state
  - Defensive exits when drawdown imminent
  - Trend-following exits when momentum continues
  - Better Sharpe ratio from balanced risk/reward
